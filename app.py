from langchain.chains import RetrievalQA
from langchain.schema import (SystemMessage, HumanMessage, AIMessage)
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
import os
import streamlit as st
from langchain.prompts import ChatPromptTemplate
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.prompts import PromptTemplate

def load_db(embeddings):
    return FAISS.load_local('faiss_store', embeddings, allow_dangerous_deserialization=True)


def init_page():
    st.set_page_config(
        page_title='Geminiã‚’æ´»ç”¨ã—ãŸRAGã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³',
        page_icon="ğŸ§‘â€ğŸ’»"
    )
    st.header('ãƒ©ãƒ©ãƒ©ãŸã‹ã²ã‚‰ã«ã¤ã„ã¦èã„ã¦ã¿ã‚ˆã†')


def main():
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001"
    )
    db = load_db(embeddings)
    init_page()

    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        temperature=0.0,
        max_retries=2,
    )

    # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®System Instructionã‚’å®šç¾©ã™ã‚‹
    prompt_template = """
    ã‚ãªãŸã¯ã€ãƒ©ãƒ©ãƒ©ãŸã‹ã²ã‚‰ã«è©³ã—ã„ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚

    ãƒ©ãƒ©ãƒ©ãŸã‹ã²ã‚‰ã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚ãƒ©ãƒ©ãƒ©ãŸã‹ã²ã‚‰ã«é–¢ä¿‚ã®ãªã„è³ªå•ã«ã¯ã€ã€Œãƒ©ãƒ©ãƒ©ãŸã‹ã²ã‚‰ã«é–¢ä¿‚ã™ã‚‹ã“ã¨ã«ã¤ã„ã¦èã„ã¦ãã ã•ã„ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

    ä»¥ä¸‹ã®èƒŒæ™¯æƒ…å ±ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚èƒŒæ™¯æƒ…å ±ã«ãªã„æƒ…å ±ã‚’å‹æ‰‹ã«ä½œæˆã—ã¦å™“ã‚’ã¤ã‹ãªã„ã§ãã ã•ã„
    # èƒŒæ™¯æƒ…å ±
    {context}

    #è³ªå•
    {question}"""
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=db.as_retriever(),
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
    )

    if "messages" not in st.session_state:
      st.session_state.messages = []
    if user_input := st.chat_input('è³ªå•ã—ã‚ˆã†ï¼'):
        # ä»¥å‰ã®ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’è¡¨ç¤º
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        print(user_input)
        with st.chat_message('user'):
            st.markdown(user_input)
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message('assistant'):
            with st.spinner('Gemini is typing ...'):
                response = qa.invoke(user_input)
            st.markdown(response['result'])
            st.markdown('---')
            st.markdown('**ã‚½ãƒ¼ã‚¹**')
            st.markdown(response['source_documents'])
            st.markdown('---')
        st.session_state.messages.append({"role": "assistant", "content": response["result"]})


if __name__ == "__main__":
  main()