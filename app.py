#ãƒ©ãƒ©ãƒ©ãŸã‹ã²ã‚‰ã«ã¤ã„ã¦ã®è³ªå•ã«ç­”ãˆã‚‹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’ä½œæˆã—ã¦ã„ã¾ã™ã€‚
from langchain.chains import RetrievalQA
from langchain.schema import (SystemMessage, HumanMessage, AIMessage)
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
import os
import streamlit as st
from langchain.prompts import ChatPromptTemplate
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.prompts import PromptTemplate

def load_db(embeddings):
    return FAISS.load_local('faiss_store', embeddings, allow_dangerous_deserialization=True)


def init_page():
    st.set_page_config(
        page_title='ãƒ©ãƒ©ãƒ©ãŸã‹ã²ã‚‰ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ',
        page_icon="ğŸ§‘â€ğŸ’»"
    )
    st.header('ãƒ©ãƒ©ãƒ©ãŸã‹ã²ã‚‰ã«ã¤ã„ã¦èã„ã¦ã¿ã‚ˆã†')
    st.markdown(
    '<p style="font-size:12px;">ã“ã¡ã‚‰ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¯ãƒ©ãƒ©ãƒ©ãŸã‹ã²ã‚‰ã®å›£ä½“ã«ã¤ã„ã¦ç­”ãˆã¾ã™ãŒå›ç­”ã¯å¿…ãšã—ã‚‚æ­£ã—ã„ã¨ã¯é™ã‚Šã¾ã›ã‚“ã€‚è©³ã—ãã¯<a href="https://lalala-takahira.github.io/homepage/" target="_blank">å…¬å¼ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸</a>ã€<a href="https://www.instagram.com/lalala_takahira/" target="_blank">ã‚¤ãƒ³ã‚¹ã‚¿ã‚°ãƒ©ãƒ </a>ã‚’ã”è¦§ãã ã•ã„ã€‚</p>',
    unsafe_allow_html=True
    )


def main():
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001"
    )
    db = load_db(embeddings)
    init_page()

    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        temperature=0.0,
        max_retries=2,
    )

    # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®System Instructionã‚’å®šç¾©ã™ã‚‹
    prompt_template = """
    ã‚ãªãŸã¯ã€ãƒ©ãƒ©ãƒ©ãŸã‹ã²ã‚‰ã«è©³ã—ã„ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚

    ãƒ©ãƒ©ãƒ©ãŸã‹ã²ã‚‰ã«é–¢ã™ã‚‹è³ªå•ã«ã€èƒŒæ™¯æƒ…å ±ã‚’å‚è€ƒã«ç­”ãˆã¦ãã ã•ã„ã€‚
    ãƒ©ãƒ©ãƒ©ãŸã‹ã²ã‚‰ã«å…¨ç„¶é–¢ä¿‚ã®ãªã„è³ªå•ã«ã¯ã€ã€Œãƒ©ãƒ©ãƒ©ãŸã‹ã²ã‚‰ã«é–¢ä¿‚ã™ã‚‹ã“ã¨ã«ã¤ã„ã¦èã„ã¦ãã ã•ã„ã€ã¨ã®ã¿ç­”ãˆã¦ãã ã•ã„ã€‚
    
    ã¾ãŸã€æœ€å¾Œã«è³ªå•ã®å†…å®¹ã‚’ã¿ã¦ã€ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦å‚è€ƒurlã‚’å‡ºã—ã¦ãã ã•ã„ã€‚
    ãƒ©ãƒ©ãƒ©ãŸã‹ã²ã‚‰å…¨èˆ¬ã«é–¢ã™ã‚‹ã“ã¨ã§ã‚ã‚Œã°ã€
    ã€Œãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã¯ã“ã¡ã‚‰ã‚’ã”è¦§ãã ã•ã„ã€‚ã€€https://lalala-takahira.github.io/homepage/ã€€ã€ã¨æœ€å¾Œã«ç­”ãˆã¦ãã ã•ã„ã€‚
    ã‚¤ãƒ™ãƒ³ãƒˆã®æƒ…å ±ã®æƒ…å ±ã«ã¤ã„ã¦ã®è³ªå•ãªã‚‰
    ã€Œã‚¤ãƒ™ãƒ³ãƒˆã¯ã“ã¡ã‚‰ã‚’ã”è¦§ãã ã•ã„ã€‚ã€€https://lalala-takahira.github.io/homepage/eventsã€ã¨æœ€å¾Œã«ç­”ãˆã¦ãã ã•ã„ã€‚
    éå»æ´»å‹•ã«ã¤ã„ã¦èã‹ã‚ŒãŸã‚‰ã€
    ã€Œéå»ã®æ´»å‹•ã¯ã“ã¡ã‚‰ã‚’ã”è¦§ãã ã•ã„ã€‚ã€€https://lalala-takahira.github.io/homepage/reportsã€ã¨æœ€å¾Œã«ç­”ãˆã¦ãã ã•ã„ã€‚
    ãƒ©ãƒ©ãƒ©ãŸã‹ã²ã‚‰ã«ã¤ã„ã¦è©³ã—ãã—çŸ¥ã‚ŠãŸãã†ãªè³ªå•ã«ã¯ã€
    ã€Œãƒ©ãƒ©ãƒ©ãŸã‹ã²ã‚‰ã«ã¤ã„ã¦ã¯è©³ã—ãçŸ¥ã‚ŠãŸã„æ–¹ã¯ã“ã¡ã‚‰ã‚’ã”è¦§ãã ã•ã„ã€‚ã€€https://lalala-takahira.github.io/homepage/aboutã€€ã€ã¨æœ€å¾Œã«ç­”ãˆã¦ãã ã•ã„ã€‚
    ãƒ¡ãƒ‡ã‚£ã‚¢æ²è¼‰ã«èã‹ã‚ŒãŸã‚‰
    ã€Œã“ã¡ã‚‰ã‚’ã”è¦§ãã ã•ã„ã€‚ã€€https://lalala-takahira.github.io/homepage/mediaã€ã¨æœ€å¾Œã«ç­”ãˆã¦ãã ã•ã„ã€‚
    ã•ã‚“ã ã¾ã¡åšã«ã¤ã„ã¦èã‹ã‚ŒãŸã‚‰
    ã€Œã•ã‚“ã ã¾ã¡åšã«ã¤ã„ã¦ã¯ã€ã“ã¡ã‚‰ã‚’ã”è¦§ãã ã•ã„ã€‚ã€€https://sanda-machihaku.jp/p-2024-28/ã€€ã€ã¨æœ€å¾Œã«ç­”ãˆã¦ãã ã•ã„ã€‚

    
    è³ªå•ã®å›ç­”ã«ã¯ä»¥ä¸‹ã®èƒŒæ™¯æƒ…å ±ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚èƒŒæ™¯æƒ…å ±ã«ãªã„æƒ…å ±ã‚’å‹æ‰‹ã«ä½œæˆã—ã¦å™“ã‚’ã¤ã‹ãªã„ã§ãã ã•ã„
    # èƒŒæ™¯æƒ…å ±
    {context}

    #è³ªå•
    {question}"""
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=db.as_retriever(),
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
    )

    if "messages" not in st.session_state:
      st.session_state.messages = []
    # å…¥åŠ›æ–‡å­—æ•°ã®åˆ¶é™ã‚’è¨­å®š
    max_length = 100
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›
    user_input = st.chat_input('è³ªå•ã—ã‚ˆã†ï¼')

    # å…¥åŠ›ãŒã‚ã‚‹å ´åˆã®å‡¦ç†
    if user_input:
        char_count = len(user_input)

        # 100æ–‡å­—ã‚’è¶…ãˆãŸå ´åˆã®è­¦å‘Š
        if char_count > max_length:
            st.warning(f'å…¥åŠ›ã¯{max_length}æ–‡å­—ä»¥å†…ã«ã—ã¦ãã ã•ã„ã€‚ç¾åœ¨ã®æ–‡å­—æ•°: {char_count}')
        else:
            # ä»¥å‰ã®ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’è¡¨ç¤º
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])
            with st.chat_message('user'):
                st.markdown(user_input)
            st.session_state.messages.append({"role": "user", "content": user_input})
            with st.chat_message('assistant'):
                with st.spinner('å›ç­”ã‚’å–å¾—ä¸­...'):
                    response = qa.invoke(user_input)
                st.markdown(response['result'])
            st.session_state.messages.append({"role": "assistant", "content": response["result"]})

if __name__ == "__main__":
    main()